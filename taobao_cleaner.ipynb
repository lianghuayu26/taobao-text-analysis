{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71cc95d2-681e-453d-af83-14a3f2c29819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据清洗和预处理完成，结果已保存到output目录\n",
      "清洗后的数据保存在: D:\\jupyter_project\\individual_assignment\\taobao\\cleaned_taobao_reviews.csv\n",
      "清洗前后的文本对比示例保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_cleaning_samples.txt\n",
      "文本长度统计保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_length_stats.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# 创建目录\n",
    "output_dir = r'D:\\jupyter_project\\individual_assignment\\taobao'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 下载必要的NLTK资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'taobao_reviews_us_en.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 保存原始数据副本\n",
    "df_original = df.copy()\n",
    "\n",
    "# 文本清洗函数\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 移除用户名标记\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 移除特殊字符和标点符号，但保留句子结构\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?]', '', text)\n",
    "    \n",
    "    # 移除数字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 移除多余的空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 应用文本清洗\n",
    "df['清洗后评论'] = df['评论内容'].apply(clean_text)\n",
    "\n",
    "# 分词和去除停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # 分词\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 去除停用词\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "df['分词结果'] = df['清洗后评论'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# 词形还原\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "df['词形还原'] = df['分词结果'].apply(lemmatize_text)\n",
    "\n",
    "# 将词形还原后的词转回文本\n",
    "df['处理后文本'] = df['词形还原'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 保存清洗后的数据\n",
    "cleaned_file_path = os.path.join(output_dir, 'cleaned_taobao_reviews.csv')\n",
    "df[['评论ID', '用户名', '评论内容', '清洗后评论', '处理后文本', '评分', '点赞数', '评论时间']].to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# 输出清洗前后的对比示例\n",
    "sample_size = 5\n",
    "sample_indices = np.random.choice(df.shape[0], sample_size, replace=False)\n",
    "samples = df.iloc[sample_indices]\n",
    "\n",
    "# 第一个文件：样本输出\n",
    "with open(os.path.join(output_dir, 'text_cleaning_samples.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"文本清洗前后对比示例\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, row in samples.iterrows():\n",
    "        f.write(f\"示例 {i+1}:\\n\")\n",
    "        f.write(f\"原始文本: {row['评论内容']}\\n\")\n",
    "        f.write(f\"清洗后文本: {row['清洗后评论']}\\n\")\n",
    "        f.write(f\"分词结果: {', '.join(row['分词结果'])}\\n\")\n",
    "        f.write(f\"最终处理文本: {row['处理后文本']}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 统计清洗前后的文本长度变化\n",
    "df['原始长度'] = df['评论内容'].apply(len)\n",
    "df['清洗后长度'] = df['清洗后评论'].apply(len)\n",
    "df['处理后长度'] = df['处理后文本'].apply(len)\n",
    "\n",
    "length_stats = {\n",
    "    '原始文本': {\n",
    "        '平均长度': df['原始长度'].mean(),\n",
    "        '最小长度': df['原始长度'].min(),\n",
    "        '最大长度': df['原始长度'].max()\n",
    "    },\n",
    "    '清洗后文本': {\n",
    "        '平均长度': df['清洗后长度'].mean(),\n",
    "        '最小长度': df['清洗后长度'].min(),\n",
    "        '最大长度': df['清洗后长度'].max()\n",
    "    },\n",
    "    '处理后文本': {\n",
    "        '平均长度': df['处理后长度'].mean(),\n",
    "        '最小长度': df['处理后长度'].min(),\n",
    "        '最大长度': df['处理后长度'].max()\n",
    "    }\n",
    "}\n",
    "\n",
    "# 建议文件名和路径全用英文\n",
    "# 第二个文件：cleaned_samples.txt\n",
    "with open(\"cleaned_samples.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in samples.iterrows():\n",
    "        f.write(f\"示例 {i+1}:\\n\")\n",
    "        f.write(f\"原始文本: {row['评论内容']}\\n\")\n",
    "        f.write(f\"清洗后文本: {row['清洗后评论']}\\n\")\n",
    "        f.write(f\"分词结果: {', '.join(row['分词结果'])}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"数据清洗和预处理完成，结果已保存到output目录\")\n",
    "print(f\"清洗后的数据保存在: {cleaned_file_path}\")\n",
    "print(f\"清洗前后的文本对比示例保存在: {os.path.join(output_dir, 'text_cleaning_samples.txt')}\")\n",
    "print(f\"文本长度统计保存在: {os.path.join(output_dir, 'text_length_stats.txt')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751b2635-7800-41ef-bfbe-c76860f2b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c663fb8b-b20b-4d29-b514-19ba4dd61f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to D:\\Anaconda3\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd93d81-64ef-4b13-a837-4feb88baebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据清洗和预处理完成，结果已保存到output目录\n",
      "清洗后的数据保存在: D:\\jupyter_project\\individual_assignment\\taobao\\cleaned_taobao_reviews.csv\n",
      "清洗前后的文本对比示例保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_cleaning_samples.txt\n",
      "文本长度统计保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_length_stats.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import string\n",
    "# 创建目录\n",
    "output_dir = r'D:\\jupyter_project\\individual_assignment\\taobao'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 读取数据\n",
    "file_path = 'taobao_reviews_us_en.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 保存原始数据副本\n",
    "df_original = df.copy()\n",
    "\n",
    "# 文本清洗函数\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 移除用户名标记\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 移除特殊字符和标点符号，但保留句子结构\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?]', '', text)\n",
    "    \n",
    "    # 移除数字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 移除多余的空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 应用文本清洗\n",
    "df['清洗后评论'] = df['评论内容'].apply(clean_text)\n",
    "\n",
    "# 简单分词（不使用NLTK）\n",
    "def simple_tokenize(text):\n",
    "    if not isinstance(text, str) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # 简单分词：按空格分割\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 简单过滤：移除长度为1的词和常见英文停用词\n",
    "    common_stopwords = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "                        'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n",
    "                        'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "                        'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "                        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "                        'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "                        'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "                        'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "                        'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', \n",
    "                        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "                        'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'}\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in common_stopwords and len(word) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "df['分词结果'] = df['清洗后评论'].apply(simple_tokenize)\n",
    "\n",
    "# 将分词结果转回文本\n",
    "df['处理后文本'] = df['分词结果'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 保存清洗后的数据\n",
    "cleaned_file_path = os.path.join(output_dir, 'cleaned_taobao_reviews.csv')\n",
    "df[['评论ID', '用户名', '评论内容', '清洗后评论', '处理后文本', '评分', '点赞数', '评论时间']].to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# 输出清洗前后的对比示例\n",
    "sample_size = 5\n",
    "sample_indices = np.random.choice(df.shape[0], sample_size, replace=False)\n",
    "samples = df.iloc[sample_indices]\n",
    "\n",
    "with open(os.path.join(output_dir, 'text_cleaning_samples.txt'), 'w', encoding='utf-8') as f:\n",
    "\n",
    "    f.write(\"文本清洗前后对比示例\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, row in samples.iterrows():\n",
    "        f.write(f\"示例 {i+1}:\\n\")\n",
    "        f.write(f\"原始文本: {row['评论内容']}\\n\")\n",
    "        f.write(f\"清洗后文本: {row['清洗后评论']}\\n\")\n",
    "        f.write(f\"分词结果: {', '.join(row['分词结果'])}\\n\")\n",
    "        f.write(f\"最终处理文本: {row['处理后文本']}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "# 统计清洗前后的文本长度变化\n",
    "df['原始长度'] = df['评论内容'].apply(len)\n",
    "df['清洗后长度'] = df['清洗后评论'].apply(len)\n",
    "df['处理后长度'] = df['处理后文本'].apply(len)\n",
    "\n",
    "length_stats = {\n",
    "    '原始文本': {\n",
    "        '平均长度': df['原始长度'].mean(),\n",
    "        '最小长度': df['原始长度'].min(),\n",
    "        '最大长度': df['原始长度'].max()\n",
    "    },\n",
    "    '清洗后文本': {\n",
    "        '平均长度': df['清洗后长度'].mean(),\n",
    "        '最小长度': df['清洗后长度'].min(),\n",
    "        '最大长度': df['清洗后长度'].max()\n",
    "    },\n",
    "    '处理后文本': {\n",
    "        '平均长度': df['处理后长度'].mean(),\n",
    "        '最小长度': df['处理后长度'].min(),\n",
    "        '最大长度': df['处理后长度'].max()\n",
    "    }\n",
    "}\n",
    "\n",
    "# 用 utf-8 编码写入，支持 emoji 和所有字符\n",
    "with open(\"cleaned_samples.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in samples.iterrows():\n",
    "        f.write(f\"示例 {i+1}:\\n\")\n",
    "        f.write(f\"原始文本: {row['评论内容']}\\n\")\n",
    "        f.write(f\"清洗后文本: {row['清洗后评论']}\\n\")\n",
    "        f.write(f\"分词结果: {', '.join(row['分词结果'])}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"数据清洗和预处理完成，结果已保存到output目录\")\n",
    "print(f\"清洗后的数据保存在: {cleaned_file_path}\")\n",
    "print(f\"清洗前后的文本对比示例保存在: {os.path.join(output_dir, 'text_cleaning_samples.txt')}\")\n",
    "print(f\"文本长度统计保存在: {os.path.join(output_dir, 'text_length_stats.txt')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caf11b0-35f0-472a-8f2b-e9334802b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据清洗和预处理完成，结果已保存到 taobao 目录\n",
      "清洗后的数据保存在: D:\\jupyter_project\\individual_assignment\\taobao\\cleaned_taobao_reviews.csv\n",
      "清洗前后的文本对比示例保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_cleaning_samples.txt\n",
      "文本长度统计保存在: D:\\jupyter_project\\individual_assignment\\taobao\\text_length_stats.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"数据清洗和预处理完成，结果已保存到 taobao 目录\")\n",
    "print(f\"清洗后的数据保存在: {os.path.join(output_dir, 'cleaned_taobao_reviews.csv')}\")\n",
    "print(f\"清洗前后的文本对比示例保存在: {os.path.join(output_dir, 'text_cleaning_samples.txt')}\")\n",
    "print(f\"文本长度统计保存在: {os.path.join(output_dir, 'text_length_stats.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ed38ea-0ba6-4bfd-9f9d-d156033ec6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是输出目录下的文件：\n",
      "✅ .ipynb_checkpoints\n",
      "✅ Aclean2.1l去标点.ipynb\n",
      "✅ cleaned_samples.txt\n",
      "✅ cleaned_taobao_reviews.csv\n",
      "✅ taobao_cleaner.ipynb\n",
      "✅ taobao_crawler.ipynb\n",
      "✅ taobao_reviews_cn_zh.csv\n",
      "✅ taobao_reviews_us_en.csv\n",
      "✅ text_cleaning_samples.txt\n",
      "✅ 整理数据.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"以下是输出目录下的文件：\")\n",
    "for f in os.listdir(output_dir):\n",
    "    print(\"✅\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab09726-14b9-4d6d-ab55-a24fa93d443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据示例:\n",
      "                                   评论ID                        用户名  \\\n",
      "0  fa046802-4a1c-45d3-84a6-68c02e875ed5  Maria Nelly Arevalo Toala   \n",
      "1  588015dc-9dfe-408f-8bdf-f6a3c274780d              Rayna Maschan   \n",
      "2  342b6a5f-1f2d-4ecb-af6a-35b3da4116e7                 Celina Lee   \n",
      "3  bd012696-c2d0-436c-a163-970b89e8ae21                       Greg   \n",
      "4  48835ad9-b550-4a5d-8775-7268349d4e01                 Linda Wong   \n",
      "\n",
      "                                                评论内容  \\\n",
      "0  I've been using this app for about a month, an...   \n",
      "1  Language Barrier and unfortunately, the only l...   \n",
      "2  Keeps having you sign in repeatedly then claim...   \n",
      "3  It's pretty satisfying and cheap. Only thing i...   \n",
      "4  I bought more than two hundred products since ...   \n",
      "\n",
      "                                               清洗后评论  \\\n",
      "0  ive been using this app for about a month, and...   \n",
      "1  language barrier and unfortunately, the only l...   \n",
      "2  keeps having you sign in repeatedly then claim...   \n",
      "3  its pretty satisfying and cheap. only thing is...   \n",
      "4  i bought more than two hundred products since ...   \n",
      "\n",
      "                                               处理后文本  评分   点赞数  \\\n",
      "0  ive using app month, options good. however, ma...   2    37   \n",
      "1  language barrier unfortunately, language optio...   3   217   \n",
      "2  keeps sign repeatedly claims security risks ac...   1   268   \n",
      "3  pretty satisfying cheap. thing though ask sign...   2   149   \n",
      "4  bought two hundred products since last year fr...   5  1000   \n",
      "\n",
      "                  评论时间                                     cleaned_review  \n",
      "0  2025-05-15 07:51:21  ive using app month options good however main ...  \n",
      "1  2025-04-14 08:45:44  language barrier unfortunately language option...  \n",
      "2  2025-04-12 02:08:13  keeps sign repeatedly claims security risks ac...  \n",
      "3  2025-03-11 05:57:00  pretty satisfying cheap thing though ask sign ...  \n",
      "4  2025-04-21 16:23:18  bought two hundred products since last year fr...  \n",
      "已移除列 '清洗后评论' 中的标点符号\n",
      "已移除列 '处理后文本' 中的标点符号\n",
      "清洗后的数据已保存为 cleaned_taobao_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# 读取原始数据\n",
    "input_file = 'cleaned_taobao_reviews.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 查看前几行数据\n",
    "print(\"原始数据示例:\")\n",
    "print(df.head())\n",
    "\n",
    "# 定义函数：移除标点符号\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "# 应用清洗函数到多个列\n",
    "columns_to_clean = ['清洗后评论', '处理后文本']\n",
    "for col in columns_to_clean:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(remove_punctuation)\n",
    "        print(f\"已移除列 '{col}' 中的标点符号\")\n",
    "    else:\n",
    "        print(f\"未找到列：'{col}'，请确认列名\")\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file = 'cleaned_taobao_reviews.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"清洗后的数据已保存为 {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf9969-70df-4b9f-acb7-f9307a470a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7685b-d9eb-4eca-a0c7-5168a6982112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372b2f3-6452-4c06-93b6-4ab8fcf27cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50fb4a-2b11-4325-bf34-9a99610c419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
